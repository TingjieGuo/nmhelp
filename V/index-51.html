<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<TITLE>Page 51</TITLE>

<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<DIV style="position:relative;width:612;height:792;">
<STYLE type="text/css">
<!--
	.ft0{font-size:9px;font-family:Times;color:#000000;}
	.ft1{font-size:9px;font-family:Times;color:#000000;}
	.ft2{font-size:9px;font-family:Times;color:#000000;}
	.ft3{font-size:9px;font-family:Symbol;color:#000000;}
	.ft4{font-size:8px;font-family:Symbol;color:#000000;}
	.ft5{font-size:5px;font-family:Times;color:#000000;}
	.ft6{font-size:9px;line-height:13px;font-family:Times;color:#000000;}
-->
</STYLE>
</HEAD>
<BODY bgcolor="#A0A0A0" vlink="blue" link="blue">
<IMG width="612" height="792" src="index051.png" alt="background image">
<DIV style="position:absolute;top:41;left:214"><nobr><span class="ft0">Chapter 5 - Estimates, Conﬁdence Intervals, and Hypothesis Tests</span></nobr></DIV>
<DIV style="position:absolute;top:77;left:72"><nobr><span class="ft6">parameter is required. In many situations, LS estimators of ﬁxed effects parameters are unbiased,<br>although in nonlinear problems, such as most pharmacokinetic ones, this cannot be assured.</span></nobr></DIV>
<DIV style="position:absolute;top:120;left:72"><nobr><span class="ft1"><b>4.1. Distribution of Parameters vs Distribution of Parameter Estimates</b></span></nobr></DIV>
<DIV style="position:absolute;top:137;left:72"><nobr><span class="ft6">Figure 5.1 illustrates the distribution of parameter estimates that might result if an experiment<br>were replicated. The bias and spread depend on the estimation method, the design of the experi-<br>ment (<i>x</i>, which implicitly includes <i>n</i>) and on the true parameter values, including the variances<br>(and covariances) of the random effects inﬂuencing <i>y</i>. If, for example, more observations were<br>obtained in each experiment (more individuals in a population study), the spread of the estimates<br>(one from each experiment) would decrease until, in the limit, if an inﬁnite number of observa-<br>tions were obtained in each experiment, every estimate would be the same (equal to the true value<br>plus the bias of the estimator). Thus, the distribution of the estimate tells us nothing about biol-<br>ogy or measurement error, but only about the <i>precision </i>of the estimate itself.</span></nobr></DIV>
<DIV style="position:absolute;top:257;left:72"><nobr><span class="ft0">In contrast,</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:125"><nobr><span class="ft3">Ω</span></nobr></DIV>
<DIV style="position:absolute;top:257;left:136"><nobr><span class="ft0">and</span></nobr></DIV>
<DIV style="position:absolute;top:254;left:155"><nobr><span class="ft3">Σ</span></nobr></DIV>
<DIV style="position:absolute;top:257;left:165"><nobr><span class="ft0">tell us about unexplained (or random) interindividual variability (biology) or</span></nobr></DIV>
<DIV style="position:absolute;top:270;left:72"><nobr><span class="ft6">error magnitude (biology plus measurement error), not about how precisely we know these things.<br>No matter how many observations we make, interindividual variability will remain the same size<br>(but the variability of our estimate of its size will decrease), as will the measurement error vari-<br>ability of a particular instrument.</span></nobr></DIV>
<DIV style="position:absolute;top:326;left:72"><nobr><span class="ft6">It is very important not to confuse variability (e.g., between individuals) in a model parameter<br>with variability in the estimate of that parameter, despite the fact that the terms we use to describe<br>both variabilities are similar. Thus an element of</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:298"><nobr><span class="ft4">η</span></nobr></DIV>
<DIV style="position:absolute;top:352;left:305"><nobr><span class="ft0">, say</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:329"><nobr><span class="ft4">η</span></nobr></DIV>
<DIV style="position:absolute;top:352;left:336"><nobr><span class="ft0">1 has a <i>variance</i>,</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:417"><nobr><span class="ft4">ω</span></nobr></DIV>
<DIV style="position:absolute;top:356;left:425"><nobr><span class="ft5">11</span></nobr></DIV>
<DIV style="position:absolute;top:352;left:433"><nobr><span class="ft0">, while the esti-</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:72"><nobr><span class="ft0">mate of</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:109"><nobr><span class="ft4">ω</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:117"><nobr><span class="ft5">11</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:125"><nobr><span class="ft0">, ˆ</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:131"><nobr><span class="ft4">ω</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:139"><nobr><span class="ft5">11</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:147"><nobr><span class="ft0">, also has a <i>variance </i>given by the square of the standard error for ˆ</span></nobr></DIV>
<DIV style="position:absolute;top:362;left:446"><nobr><span class="ft4">ω</span></nobr></DIV>
<DIV style="position:absolute;top:369;left:454"><nobr><span class="ft5">11</span></nobr></DIV>
<DIV style="position:absolute;top:365;left:462"><nobr><span class="ft0">. Indeed,</span></nobr></DIV>
<DIV style="position:absolute;top:378;left:72"><nobr><span class="ft6">the use of the term &quot;standard error&quot; rather than &quot;standard deviation&quot; to name a measure of the<br>spread in the distribution of the parameter <i>estimate </i>rather than in the parameter helps call atten-<br>tion to the distinction between these two types of things.</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:72"><nobr><span class="ft1"><b>4.2. Conﬁdence Interval for a Single Parameter</b></span></nobr></DIV>
<DIV style="position:absolute;top:446;left:72"><nobr><span class="ft6">Acknowledging that any particular estimate from any particular experiment is unlikely to equal<br>the true parameter value implies that we should be interested in &quot;interval&quot; estimates of parameters<br>as well as (instead of?) point estimates. An interval estimate of a parameter is usually a range of<br>values for the parameter, often centered at the point estimate, such that the range contains the true<br>parameter value with a speciﬁed probability. The probability chosen is often 95%, in which case<br>the interval estimate is called the 95% Conﬁdence Interval (CI).</span></nobr></DIV>
<DIV style="position:absolute;top:528;left:72"><nobr><span class="ft6">A CI is often based only on the parameter estimate and its <i>SE</i>. In the next sections we discuss<br>three questions about such CIs a little further. (i) How to estimate the <i>SE </i>from a single set of data<br>(we cannot replicate our experiment many times and construct a histogram as in ﬁgure 5.1). (ii)<br>Given an estimate of <i>SE</i>, how to use that number to compute a (95% conﬁdence) interval with<br>95% chance of containing the true parameter value. (iii) Given an estimate of <i>SE</i>, how to com-<br>pute a conﬁdence interval for a function of the parameter.</span></nobr></DIV>
<DIV style="position:absolute;top:619;left:72"><nobr><span class="ft1"><b>4.2.1. Estimating a Parameter’s Standard Error</b></span></nobr></DIV>
<DIV style="position:absolute;top:636;left:72"><nobr><span class="ft6">Remarkably, the <i>SE </i>of a parameter estimate can be estimated using only the data from a single<br>experiment. The idea is that the data give us estimates of the variances of all random effects in<br>our model, from which we can estimate the variability in future data (if we were to replicate the<br>experiment). That is, the SE of the estimates on replication depends only on quantities we either<br>know or hav e estimates of: the <i>x</i>, the number of <i>y </i>observed (<i>n</i>), and the variances of all random<br>effects.</span></nobr></DIV>
<DIV style="position:absolute;top:761;left:279"><nobr><span class="ft0">-43-</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
